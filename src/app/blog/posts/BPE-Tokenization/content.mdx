Neural networks consume information in a numerical form organized into tensors. This is not necessarily an optimal representation of information or a reflection of how we humans process it, but rather a practical representation dictated by the capabilities of our current hardware and software paradigms.

In Computer Vision for example we assume that pixels are meaningful statistical units of images. We are essentially imposing a discrete grid on continuous visual information. We lose information about sub-pixel details, we make arbitrary choices about colour spaces and bit depths, and we force everything into rectangular grids even though that's not how biological vision works. But it's good enough, it works well with our architectures and we rarely see it discussed as a bottleneck.

In Natural Language Processing (NLP), the input is text. The primary challenge is to convert this symbolic, sequential data into a meaningful numerical format. This process is called **tokenization**.

There are different ways to approach this. All involve constructing a look-up table, or vocabulary, that maps a chosen *granularity of text* to a unique integer $ID$. This $ID$ is then mapped to a **learnable embedding vector**.

The choice of granularity; bytes, characters, sub-words, or words is not a trivial implementation detail. It is a fundamental decision that deeply influences the model's performance for two primary reasons: **computational efficiency** and **learnability**.

### Efficiency and The Compression Ratio:

A model's computational cost, particularly for attention-based architectures like the Transformer, is highly sensitive to sequence length. The attention mechanism has a complexity that scales quadratically with the sequence length, $O(n^2)$, where $n$ is the number of tokens.

This places a premium on representing text with the fewest tokens possible without losing essential information. We can quantify this with the **compression ratio**, or its inverse, **Bytes per Token**:

$$\text{Bytes per Token}= \frac{\text{Number of Bytes in Original Text}}{\text{Number of Tokens}}$$

A lower number of tokens for a given text results in shorter sequences, which means faster training and inference, reduced memory consumption, and the ability to process longer contexts.

Let's take an example, say we want to tokenize the following:

```
The apparent meaning is for the masses, and the inner meaning is for the learned.
```

The previous text has **93 characters**. If we assume each character is 1 byte (as in $ASCII$ or $UTF-8$ for this basic text), then it's **93 bytes**.

- **Word-level tokenization** offers an excellent compression ratio. For the previous example we  would get $15$ tokens (Separated by weight space), which corresponds to a compression ratio of around:
  $$\text{Bytes per Token} = \frac{93}{15} = 6.2\ \text{Bytes/Token}$$
  This is excellent and solves the efficiency problem, however, it struggles with morphological variants (e.g., "run", "runs", "running" are three separate tokens), this leads to a very large vocabulary (Look-up table we need to store) and fails completely on out-of-vocabulary ($OOV$) words since we can never have all words, forcing the use of a generic `<UNK>` token that discards information.
- **Byte-level tokenization** now this solves the $OOV$ problem, as any text can be represented as a sequence of bytes. However, it has a terrible compression ratio, leading to extremely long sequences and making it computationally expensive, in  the previous text we would get:
  $$\text{Bytes per Token} = \frac{93}{93} = 1\ \text{Bytes/Token}$$
  This is the worst compression ratio we can have, there's no compression.

The question here becomes how to find a middle ground between the two: have a decent compression ratio while preserving the ability to represent arbitrary words.

Now if you have any background in Computer Science, you probably thought of information theory, Shannon and friends, the moment I started talking about compression ratio. That's a good instinct. Deep learning, as a relatively young field, frequently leverages insights from established disciplines, a form of intellectual arbitrage you'll find everywhere.

Back to information theory: we know there's a theoretical minimum number of bits to represent information losslessly, quantified by **Shannon entropy**. Lossless compression algorithms like **Huffman coding** approach this by assigning shorter codes to more frequent symbols.

This directly applies to tokenization, although, in a way, inversely. While Huffman coding takes larger, meaningful symbols and breaks them down into optimal, shorter bit sequences, **subword tokenization** does the opposite. It builds up from the most granular units (characters or bytes) by merging the most frequent sequences into larger, more semantically meaningful subword tokens.

This form of subword tokenization leads to a better compression ratio than byte-level, and since we learn how to construct the subwords we are guaranteed to have a decent compression ratio, the other benefit is **coverage** since any word can be decomposed into its subword units, effectively eliminating the $OOV$ problem.

### Learnability and Inductive Bias

The $OOV$ problem of word-level tokenization totally takes it out of consideration for the best way to tokenize and that's fair.

Now on the other hand, a lot of people are still not sold on subword tokenization and are actively trying to find a way to directly use byte-level tokenization a nod to pixels in images. The thing is for such thing to be possible, we need a new architecture all together. Beyond efficiency, tokenization is a powerful mechanism for injecting an **inductive bias** into the model. An inductive bias is a set of assumptions the model uses to generalize from finite training data to unseen examples.

The Transformer architecture famously has a weak inductive bias but the way we segment the text itself provides a fundamental bias.

The key insight is that **sample complexity** - how many examples you need to learn a concept - depends heavily on the representation space and the hypothesis class we're searching over. Just because two tokenization schemes can represent the same information doesn't mean they're equally learnable.

**Byte-level tokenization** imposes a very weak inductive bias. The model is given a long stream of bytes and must learn from scratch that sequences like `(100, 111, 103)` (the bytes for "dog") form a coherent semantic unit. This creates a massive hypothesis space for the model to search through, potentially requiring more data (**sample complexity**), computation and **regularization**  to converge on good solutions.

 Furthermore, language modelling on byte sequences is difficult because the longer input sequences create very long-term dependencies in the data, although attention can mitigate that, research shows that we are still constrained by a certain context-length.

Now on the other hand, **sub-word tokenization** introduces a stronger, more useful inductive bias. By pre-chunking the text into statistically common sub-units (e.g., `token`, `ization`), we are giving the model a head start. We are constraining the hypothesis space, suggesting that these chunks are likely to be meaningful semantic building blocks. This is analogous to how Convolutional Neural Networks ($CNNs$) use the inductive bias of locality and spatial invariance, or how Vision Transformers ($ViTs$) first break an image into patches.

This brief introduction shows that both efficiency and learnability point towards sub-word tokenization as the superior strategy, and Byte-Pair Encoding ($BPE$) is arguably the most prevalent algorithm for achieving it.

### Byte-Pair Encoding (BPE): Compression as Inductive Bias

Byte-Pair Encoding ($BPE$)  is a greedy data compression algorithm It operates on a simple principle: **iteratively merge the most frequent adjacent pairs of tokens in a corpus**, building up a vocabulary of subword units.

Now if you have any background in Computer Science, you probably thought of information theory, Shannon and friends, the moment I started talking about compression. That's a good instinct. Deep learning, as a relatively young field, frequently leverages insights from established disciplines, a form of intellectual arbitrage you'll find everywhere.

Back to information theory: we know there's a theoretical minimum number of bits to represent information losslessly, quantified by **Shannon entropy**. Lossless compression algorithms like **Huffman coding** approach this by assigning shorter codes to more frequent symbols.

This directly applies to tokenization, although, in a way, inversely. While Huffman coding takes larger, meaningful symbols and breaks them down into optimal, shorter bit sequences, **subword tokenization** does the opposite. It builds up from the most granular units (characters or bytes) by merging the most frequent sequences into larger, more semantically meaningful subword tokens.

To learn the subword units or merges, we need to train our $BPE$ on a large corpus of text that is representative of the text our transformer will encounter during training. This means that the merges we learn from a given corpus reflect the statistical structure of that corpus. Now since that corpus is a representative sample of the data the language model will be trained on, then BPE's merge rules become a distilled fingerprint of that distribution. This raises a subtle point: **can we infer the data composition of a model’s training data simply by analysing its tokenizer's merges ?**

Let's now see how to the $BPE$ training procedure unfolds with code of a naïve implementation, in this section we will follow the algorithm step-by-step without much thought about wall-clock time

Now the $BPE$ training procedure unfolds in several key stages.

###### 1. **Vocabulary Initialization**

The vocabulary begins with a flat base: the set of all possible byte values, typically the $256$ values from $0$ to $255$.

These are the atomic units. Since nearly all modern text is encoded in UTF-8, every Unicode character can ultimately be expressed as a sequence of these bytes.

This ensures total coverage, every possible input string is valid under this tokenizer. It also means that even emojis or multilingual scripts can be decomposed into legal, **if not always semantically coherent**, byte sequences.

We can simply achieve this by the following code:

```python
	vocab = {i : bytes([i]) for i in range(256)}
```

###### 2. **Special Tokens**

Before any merging happens, special tokens are injected into the vocabulary. These might include tokens like `<|endoftext|>`, `<PAD>`, or `<BOS>`. They are treated as indivisible atomic units and are _excluded_ from the merge process to preserve their role  as control signals in generation or evaluation

We would simply do:

```python
for i, token in enumerate(special_tokens):
    vocab[256 + i] = token.encode("utf-8")
```

###### 3. **Pre-tokenization: Structuring the Input**

Feeding raw text directly into the BPE algorithm creates issues. Consider the difference between `"dog"` and `"dog!"` or `"I am"` and `"I'm"`—without some initial structuring, BPE would treat these as entirely separate byte sequences, unable to connect their underlying semantics. This affects both _compression_ (we miss common patterns) and _learnability_ (we fragment meaningful concepts).

To mitigate this, most modern implementations perform **pre-tokenization**, splitting the input corpus into "word-like" units before any merging begins. The GPT-2 tokenizer, for instance, uses a carefully constructed regular expression to handle English morphology, contractions, and punctuation:

```python
PAT = r"""'(?:[sdmt]|ll|ve|re)| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""
```

This pattern groups sequences of letters (`\p{L}`), numbers (`\p{N}`), and punctuation, ensuring that merges respect linguistic boundaries. The goal isn’t perfect linguistic segmentation, but rather to seed the BPE process with more meaningful initial units, improving its downstream merges.

> Pre-tokenization is often ignored in discussions about BPE, but it plays a crucial role in shaping what the algorithm sees, and what it learns to merge, a simple difference in the regular expression can lead to substantive differences in the final merges or vocabulary.

In terms of code:

```python
def pre_tokenization(training_data, special_tokens):
    # This is taken from github.com/openai/tiktoken/pull/234/files (GPT2)
    PAT = r"""'(?:[sdmt]|ll|ve|re)| ?\p{N}+| ?\p{L}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""

    if not special_tokens:
        return [m.group() for m in re.finditer(PAT, training_data)]

    escaped_list = [re.escape(special_token) for special_token in special_tokens]
    split_PAT = r"({})".format("|".join(escaped_list))
    split_corpus = re.split(split_PAT, training_data)

    pretokenized_train_data = []
    for segment in split_corpus:
        token_list = [special_token for special_token in special_tokens if special_token in segment]
        if not segment:
            continue
        if len(token_list) >= 1:
            pretokenized_train_data.append(segment)
        else:
            for m in re.finditer(PAT, segment):
                pretokenized_train_data.append(m.group())

    return pretokenized_train_data
```

You can notice that before pre-tokenization, we split on the special tokens, this is important so that no merging can occur across the text they delimit.

Notice also that we are using `re.finditer` instead of `re.findall` to avoid storing the pre-tokenized words as we construct our mapping from pre-tokens to their counts later on.
###### 4. **Iterative Merging**

With the pre-tokenized corpus in hand, the core BPE implementation begins.

First we need to choose a structure to keep the initial word counts. It is convenient to represent this as a `dict[tuple[bytes], int]`, e.g. `{(l,o,w): 5 ...}`, this facilitates the count of adjacent pairs and the merging process, this is way more efficient than storing everything separately and we use tuples because they are immutable.

The code can look something like this:

```python
merges = []
pretokenized_train_data = pre_tokenization(training_data, special_tokens)
constructed_vocab = {}
ctr = Counter(pretokenized_train_data)

for word, count in ctr.items():
    if len(word) == 1: continue
    constructed_vocab[tuple(word.encode("utf-8"))] = count
```

And then iteratively, until the desired vocabulary size is reached, apply these steps:

1. **Count** the frequency of all adjacent pairs of tokens across the dataset.
2. **Select** the pair with the highest frequency.
3. **Merge** this pair into a new token, and add it to the vocabulary.

Let's see how we might code this, starting with the outer loop:

```python
token_id = (len(vocab) + len(special_tokens)) - 1

# vocab_size is a hyperparameter.
while token_id < vocab_size - 1:
    best_pair = find_best_pair(constructed_vocab)
    merge_best_pair(constructed_vocab, best_pair, token_id)
    token_id += 1
    # Bookkeeping:
	merges.append(best_pair)
	vocab[token_id] = best_pair[0] + best_pair[1]
```

All of the work happens in the two functions `find_best_pair` and `merge_best_pair`

For finding the next pair to merge i.e. the one with the highest frequency, we can do the following:

```python
def find_best_pair(constructed_vocab):
        potential_merges_count = {}
        for index, (word, count) in enumerate(constructed_vocab.items()):
            for i in range(len(word) - 1):
                key = (word[i], word[i+1])
                potential_merges_count[key] = potential_merges_count.get(key, 0) + count

        v = max(potential_merges_count.values())
        keys = [k for k, val in potential_merges_count.items() if val == v]
        new_merge = max(keys)
        return new_merge
```

A subtle but important detail is how we deal with tie-breaking, it might seem as a second thought  but it does actually matter, current literature suggests taking the **lexicographically greatest** pair, The comparison is not over raw byte values but over the string representations of the merged tokens (We need to keep a dictionary of the string representations to use for tie-breaking).

> This detail took me longer than I care to admit to implement correctly when I was trying to recreate $TikToken$.

Now that we have our next merge, we need to apply it to all the words that contain it in our count dictionary (`constructed_vocab`).

```python

# This is a helper function to check if a word contains the merge
def contains_subtuple(big, sub):
    exists = True
    len_big = len(big)
    for i in range(len_big - 2 + 1):
        if big[i:i + 2] == sub:
              exist = True
              break
    return exists

# This is standard, we will use it again later
def merge(word_tuple, best_pair, token_id):
	new_word_list = []
	k = 0
	while k < len(word_tuple):
	current_bigram = (word_tuple[k], word_tuple[k + 1])
	if k < len(word_tuple) - 1 and current_bigram == best_pair:
		new_word_list.append(new_token_id)
		k += 2
	else:
		new_word_list.append(word_tuple[k])
		k += 1
	return tuple(new_word_list)

def merging(constructed_vocab, new_merge, token_id):
    to_add = {}
    to_delete = []
    # iterate through ALL words
    for word in list(constructed_vocab):
        if contains_subtuple(word, new_merge):
			new_word_tuple = merge(word, new_merge, token_id)
            # Bookkeeping
            count = constructed_vocab[word]
            to_delete.append(word)
            to_add[new_word_tuple] = count
    # Updates for the dictionary
    for k in to_delete:
        del constructed_vocab[k]
    constructed_vocab.update(to_add)
```

So to summarize:

1. Initialize your vocabulary with all 256 possible bytes.
2. Find the most frequent pair of adjacent tokens in your entire corpus.
3. Merge that pair into a new token.
4. Add the new token to your vocabulary.
5. Repeat $N$ times, where $N$ is your desired number of merges.

### Profiling and Optimization:

Alright ! Finally we have a working $BPE$ tokenizer, let's profile it and see.

Profiling on a very small file (around $65kb$ of text) shows that it takes around $3s$ to run the previous implementation, this is acceptable for a naïve implementation in python, but we can do way better.

Inspecting the profiling shows that there are two bottlenecks:

+ `contains_subtuple`, for every merge, we iterate over all words and over all byte pairs to identify if we need a merge, this is inefficient, we need a way to keep track of what is where.
  This also points to a problem of how we populate the `potential_merges`, for every merge, we iterates over all byte pairs to identify the most frequent pair. However, the only pair counts that change after each merge are those that overlap with the merged pair.
+ Another bottleneck although less apparent with this small file is pre-tokenization.

Fortunately, both of these bottlenecks can be overcome with some simple solutions.

#### Bottleneck 1:

The first problem points towards a data structure issue, we can optimize both the merging step and updating the `potential_merges` dictionary step by simply keeping track of the different bigrams and the words they appear in.

Let's create a new dictionary and initially populate it , we will call it `bigram_locations`.

```python
# ...same as before.. (pretokenization, construct the intitial vocab and constructed_vocab)

potential_merges = {}
bigram_locations = defaultdict(set)

for word_tuple, count in constructed_vocab.items():
    for i in range(len(word_tuple) - 1):
        bigram = (word_tuple[i], word_tuple[i + 1])
        potential_merges[bigram] += count
        bigram_locations[bigram].add(word_tuple)
```

Now we will start the outer loop, same as before, but the merging and updating logic will be quite different.

```python
token_id = (len(vocab) + len(special_tokens)) - 1

# vocab_size is a hyperparameter.
while token_id < vocab_size - 1:
    best_pair = get_best_pair(constructed_vocab)

    merge_best_pair(constructed_vocab, best_pair, token_id)
    token_id += 1
    # Bookkeeping:
	merges.append(best_pair)
	vocab[token_id] = best_pair[0] + best_pair[1]
```

Starting with the `get_best_pair`, before we were re-constructing it with each new merge, but now, with the advanced bookkeeping logic we will implement, it will always be up-to-date and we can just simply fetch the highest frequency entry.

```python

def get_best_pair(potential_merges: dict, token_str: dict):
    max_freq = max(potential_merges.values())
    candidate_pairs = [pair for pair, freq in potential_merges.items() if freq == max_freq]

    best_pair = max(
        candidate_pairs,
        key=lambda pair: (token_str[pair[0]], token_str[pair[1]]))

    return best_pair
```

Okay, now we will move on to the more complex part.

when we merge `('t', 'h')` into a new token `257` (let's call it `'th'`), what _actually_ changes?

Let's consider the word `('t', 'h', 'e', 's', 'e')`.

- The pair `('t', 'h')` disappears. Its count goes down.
- The pair `('h', 'e')` disappears. Its count goes down.
- A **new** pair `('th', 'e')` is formed. Its count goes up.

The counts for `('e', 's')` and `('s', 'e')` are completely unaffected.

So we can do the following, we will retrieve from the `bigram_locations` dictionary all the words that contain the new merge, since they are the only ones affected, and we will loop through them, and apply the three previously mentioned steps.

```python
words_affected = list(bigram_locations[best_pair].keys())

for word_tuple in words_affected:

    if word_tuple not in constructed_vocab:
        continue

    count = constructed_vocab[word_tuple]
    # Decrement stats for all bigrams in the OLD word.
    decrement_counts(word_tuple, potential_merges, bigram_locations, count)
    # Merging
    new_word_tuple = merge(word_tuple, best_pair, new_token_id)
    # Increment stats for all bigrams in the NEW word.
    increment_counts(new_word_tuple, potential_merges, bigram_locations, count)

	# cleanup
	constructed_vocab[new_word_tuple] = count
    del constructed_vocab[word_tuple]

# final cleanup
del bigram_locations[best_pair]
```

Alright ! we are almost there, the `word_merge` function is same as before.

We will only need to look at the decrementing/incrementing functionalities which are quite straightforward.

```python

def decrement_counts(word_tuple, potential_merges, bigram_locations, count):
    for j in range(len(word_tuple) - 1):
        bigram = (word_tuple[j], word_tuple[j + 1])
        potential_merges[bigram] -= count

        if bigram in bigram_locations and word_tuple in bigram_locations[bigram]:
            bigram_locations[bigram].discard(word_tuple)
            if not bigram_locations[bigram]:
                del bigram_locations[bigram]

        if potential_merges[bigram] <= 0:
            del potential_merges[bigram]

def increment_counts(new_word_tuple, potential_merges, bigram_locations, count):
    for j in range(len(new_word_tuple) - 1):
        bigram = (new_word_tuple[j], new_word_tuple[j + 1])
        potential_merges[bigram] += count
        bigram_locations[bigram].add(new_word_tuple)
```

The code is quite straightforward, this bookkeeping allows for faster updates and access mitigating the previous bottleneck that arised from the unnecessary looping.

#### Bottleneck 2:

This second bottleneck is apparent when we deal with larger files,  we can see it in the profiling of training the $BPE$ on the `22k` validation dataset of `TinyStories` for example.

Reading the file and running our regex over it takes time but unlike the merge loop, this is an "embarrassingly parallel" task.

We can split the work across multiple CPU cores. But how do you split a massive text file? You can't just cut it at random byte offsets, you might slice a multi-byte UTF-8 character in half or split a special token like `<|endoftext|>`.

The solution is to find clean seams in the data, we can achieve this by designing a `find_chunk_boundaries` function.

```python
def find_chunk_boundaries(
    file: BinaryIO,
    desired_num_chunks: int,
    split_special_token: bytes
) -> list[int]:
    """
    Chunk the file into parts that can be counted independently.    May return fewer chunks if the boundaries end up overlapping.    """    assert isinstance(split_special_token, bytes), (
        "Must represent special token as a bytestring"
    )

    # Get total file size in bytes
    file.seek(0, os.SEEK_END)
    file_size = file.tell()
    file.seek(0)

    chunk_size = file_size // desired_num_chunks

    # Initial guesses for chunk boundary locations, uniformly spaced
    # Chunks start on previous index, don't include last index    chunk_boundaries = [i * chunk_size for i in range(desired_num_chunks + 1)]
    chunk_boundaries[-1] = file_size

    mini_chunk_size = 4096  # Read ahead by 4k bytes at a time

    for bi in range(1, len(chunk_boundaries) - 1):
        initial_position = chunk_boundaries[bi]
        file.seek(initial_position)  # Start at boundary guess
        while True:
            mini_chunk = file.read(mini_chunk_size)  # Read a mini chunk

            # If EOF, this boundary should be at the end of the file            if mini_chunk == b"":
                chunk_boundaries[bi] = file_size
                break

            # Find the special token in the mini chunk
            found_at = mini_chunk.find(split_special_token)
            if found_at != -1:
                chunk_boundaries[bi] = initial_position + found_at
                break
            initial_position += mini_chunk_size

    # Make sure all boundaries are unique, but might be fewer than desired_num_chunks
    return sorted(set(chunk_boundaries))
```


In summary:

1. It gets the total file size (`file.seek(0, os.SEEK_END)`) and calculates rough byte offsets for the number of chunks you want (e.g., one per CPU core).
2. For each rough offset, it seeks to that position in the file.
3. It then starts reading small "mini-chunks" (e.g., 4KB at a time) and scans for the _next_ occurrence of our `split_special_token` (e.g., `b"<|endoftext|>"`).
4. Once found, that precise byte location becomes our clean chunk boundary.

This ensures each parallel process gets a chunk of data that doesn't corrupt any tokens at its edges. Then, we can use Python's `multiprocessing` library to efficiently parallelize  the work:

```python
def process_chunk(file_path, start, end, special_tokens):
    with open(file_path, "rb") as f:
        f.seek(start)
        text = f.read(end - start).decode("utf-8",
                                          errors="surrogateescape")
        tokens = pre_tokenization(text, special_tokens)
        return Counter(tokens)


word_counts = Counter()

with open(file_path, 'rb') as f:
    boundaries = find_chunk_boundaries(f,
                                       desired_num_chunks=os.cpu_count(),
                                       split_special_token=b"<|endoftext|>")

chunks = [(file_path, start, end, special_tokens) for start, end in zip(boundaries[:-1], boundaries[1:])]

with mp.Pool(processes=mp.cpu_count()) as pool:
    results = pool.starmap(process_chunk, chunks)

for result in results:
    word_counts.update(result)
```

Some crucial notes for engineers:
+ we use `multiprocessing`, not `threading`, because of Python's Global Interpreter Lock (GIL). The GIL prevents multiple threads from executing Python bytecode at the same time, making `threading` useless for CPU-bound tasks. `multiprocessing` sidesteps the GIL by creating separate processes, each with its own Python interpreter and memory space.
+ Using multiprocessing in Windows is beating a dead body, the overhead of `spawn` will bottleneck any implementation, for serious work, the training should be run on `Linux`

Applying these two steps on our implementation, leads to significant efficiency gains, the profiling shows that we go from `3s` in the naïve implementation to `0.3s` in this new one.

> An even more efficient implementation would be to use Linked lists for keeping track of adjacencies, using linked lists in python is a bad idea, python will punish you for trying to do memory management yourself, so we might opt for a **Rust** or **C++** backend instead.

